# 2. Â¿QuÃ© es CSCV? (Combinatorially Symmetric Cross Validation)

Es una tÃ©cnica para evaluar estrategias o configuraciones de parÃ¡metros de forma combinatoria, usando todos los posibles cortes del histÃ³rico en train/test.

### MecÃ¡nica:

Divides tu histÃ³rico en N bloques de tiempo (ej. 8 meses â†’ 8 bloques de 1 mes).

Tomas todas las combinaciones posibles:

Un subconjunto de bloques como train (optimizar).

El resto como test (validar).

Recorres todas las combinaciones (simÃ©trico = cada bloque aparece igual de veces en train y test).

Para cada combinaciÃ³n mides la mÃ©trica (ej. Sharpe, MAR).

### ğŸ‘‰ Resultado:

Obtienes una distribuciÃ³n de rendimientos en test para cada set de parÃ¡metros.

Ves quÃ© sets son consistentes en distintos cortes temporales.

âœ… Ventaja: reduce el riesgo de â€œmejor set por pura suerteâ€.

## ğŸ”¹ 3. Â¿QuÃ© es PBO? (Probability of Backtest Overfitting)

Es un unico valor global derivada de CSCV que mide la probabilidad de que tu se de parametros para optimizar estÃ© sobreajustado.

CÃ³mo se calcula:

Con CSCV ya tienes rendimiento en train y en test para cada set de parÃ¡metros.

### Comparas:

Â¿Los parÃ¡metros que son â€œtopâ€ en train tambiÃ©n lo son en test?

Â¿O solo brillan en train pero fallan en test?

El PBO es la probabilidad de que un set que parece ganador en train acabe siendo perdedor en test.

### ğŸ‘‰ InterpretaciÃ³n:

PBO bajo (<20â€“30%) â†’ tu estrategia es probablemente robusta.

PBO alto (>50%) â†’ la estrategia estÃ¡ casi seguro sobreajustada (demasiada dependencia del histÃ³rico).

# Â¿QuÃ© devuelve la funciÃ³n?

PBO (global) te dice si el procedimiento de selecciÃ³n tiende a sobreajustar.

Datos para ajustar a un solo valor que mejor se comporta. CSCV_u_, CSCV_rank, win_rate 

### InterpretaciÃ³n aproximada PBO:

< 0.2 â†’ buena seÃ±al (baja prob. de sobreajuste).

~ 0.5 â†’ â€œcara o cruzâ€ (neutro).

> 0.7 â†’ mala pinta (probable sobreajuste).

### Interpretacion datos para "mejor valor":

Por-combo, filtra candidatos con CSCV_u_median bajo y CSCV_rank_median bajo (y poca diferencia entre mean y median).

Alto win_rate + buenos u/ranks OOS â‡’ candidato fuerte.

Alto win_rate + malos u/ranks OOS â‡’ sospecha de sobreajuste.

Bajo win_rate + buenos u/ranks OOS â‡’ estable aunque rara vez â€œparezca el nÂº1â€ en IS (suele ser material de meseta)

# main.py

## CSCV_PARTITIONS = 10

QuÃ© controla: en cuÃ¡ntos bloques temporales se parte tu histÃ³rico. En cada fold, 1 bloque es OOS y el resto es IS (tipo â€œleave-one-block-outâ€).

Efecto en calidad vs. coste:

MÃ¡s particiones â‡’ mÃ¡s comprobaciones OOS (diagnÃ³stico de sobreajuste mÃ¡s fino), pero cada bloque es mÃ¡s corto y sube el coste de cÃ³mputo.

Menos particiones â‡’ menos coste, pero seÃ±al OOS mÃ¡s ruidosa.

Reglas rÃ¡pidas (por nÂº de barras por activo):

HistÃ³rico pequeÃ±o (â‰¤ ~2.000 barras): 5â€“8 particiones.

Medio (2.000â€“20.000): 8â€“12 particiones.

Grande (â‰¥ ~20.000): 12â€“20 particiones.

Complejidad aproximada: por fold se re-simulan ~2 * top_k veces (IS + OOS). Total â‰ˆ 2 * top_k * n_partitions backtests por activo y estrategia.

## CSCV_TOP_K = 10

QuÃ© controla: cuÃ¡ntas combinaciones de parÃ¡metros (del grid) entran en el CSCV. Se seleccionan las top por la mÃ©trica elegida.

Efecto:

Subir top_k â‡’ mejor representatividad (menos riesgo de â€œganador casualâ€), pero mÃ¡s re-simulaciones.

Bajar top_k â‡’ mÃ¡s rÃ¡pido, pero la estimaciÃ³n de PBO puede volverse inestable.

Reglas rÃ¡pidas (segÃºn el tamaÃ±o del grid):

Grid pequeÃ±o (â‰¤ ~100 filas): 10â€“20.

Grid medio (100â€“500): 15â€“30.

Grid grande (â‰¥ ~500): 20â€“50 o bien ~5â€“10% del grid (lo que sea mayor).

## â€œCSCV_METRICâ€ dinÃ¡mico (cÃ³mo decide la mÃ©trica)

En tu main.py usamos un selector que elige automÃ¡ticamente la mÃ©trica con la que:

rankeamos el grid,

elegimos el ganador IS, y

medimos el ranking OOS.

El orden de preferencia es:

'Sharpe Ratio' si existe en el grid,

la mÃ©trica que pusiste en OPTIMIZE_MAXIMIZE (p. ej. 'Equity Final [$]'),

'Return [%]' si existe,

si no, la Ãºltima columna numÃ©rica disponible.

## Consejos

Si quieres que DSR tambiÃ©n funcione (y que CSCV use Sharpe), optimiza por 'Sharpe Ratio' (OPTIMIZE_MAXIMIZE = 'Sharpe Ratio') para que esa columna aparezca en el grid.

Si prefieres evaluar por Equity (p. ej. en intradÃ­a con pocos trades), deja OPTIMIZE_MAXIMIZE = 'Equity Final [$]' y CSCV usarÃ¡ esa.

La mÃ©trica asumida es de â€œmayor es mejorâ€. Para mÃ©tricas de â€œmenor es mejorâ€ (p. ej. Drawdown), no las uses como metric directamente salvo que inviertas el signo antes (nosotros, por defecto, no las elegimos como primera opciÃ³n).

## Recetas rÃ¡pidas

Setup conservador (rÃ¡pido): CSCV_PARTITIONS=8, CSCV_TOP_K=10.

Setup balanceado (recomendado): CSCV_PARTITIONS=10â€“12, CSCV_TOP_K=15â€“25.

Setup estricto (mÃ¡s caro): CSCV_PARTITIONS=15â€“20, CSCV_TOP_K=30â€“50.

Regla mental: estima el coste â‰ˆ 2 * top_k * partitions * (#activos) * (#estrategias) backtests. Ajusta para que te quepa en tiempo razonable.

# IS y OOS:

### IS (In-Sample) = â€œdentro de muestraâ€.

Es el trozo de histÃ³rico que usas para elegir/ajustar los parÃ¡metros (equivale a train). AhÃ­ haces grid search, eliges el â€œmejorâ€ por tu mÃ©trica, miras heatmaps, etc.
Clave: lo tocas para aprender; por eso estÃ¡ expuesto a sobreajuste.

### OOS (Out-of-Sample) = â€œfuera de muestraâ€.

Es el trozo que NO se usÃ³ para elegir y sirve solo para evaluar (equivale a test). Con los parÃ¡metros elegidos en IS, los fijas y re-simulas en OOS.
Clave: es tu foto â€œhonestaâ€ de cÃ³mo generaliza; no se toca para ajustar.

### En CSCV / Walk-Forward:

Partes el histÃ³rico en varios bloques (p. ej., 10).

En cada fold, tomas 1 bloque como OOS y el resto como IS (tipo leave-one-block-out).

Proceso por fold:

En IS seleccionas la mejor combinaciÃ³n (entrenas/eliges).

Con esos parÃ¡metros, evalÃºas en OOS.

Repites para todos los bloques. Si el â€œganador ISâ€ se hunde a menudo en OOS, huele a sobreajuste (PBO alto).